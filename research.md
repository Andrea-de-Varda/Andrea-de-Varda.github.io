---
layout: default
title: Research
permalink: /research/
---

# Research

## Multilingual language models

Multilingual neural language models (MNLMs) are compelling scientific objects because they provide a single parametrized system that processes many languages, allowing controlled comparisons within one architecture. Unlike human studies, MNLMs can be probed at scale, intervened on, and inspected at the unit level without measurement noise. This makes them powerful tools for testing language-general hypotheses about representation and processing. At the same time, they help counter the English-centric bias of both NLP and cognitive science by enabling analyses across diverse typologies. My work uses MNLMs to ask whether a shared representational space emerges across languages and whether such spaces explain human behavior and brain responses.

### What I’ve worked on (selected)

- **Interpretability of model representations (syntax & semantics).**  
  I use unit- and subspace-level analyses to ask whether the same latent dimensions encode the same linguistic functions across languages (e.g., number agreement; affective semantics) and where in the network these signals peak.  
  _Representative papers:_  
  – *Data-driven Cross-lingual Syntax: An Agreement Study with Massively Multilingual Models* (Computational Linguistics, 2023)  
  – *The Emergence of Semantic Units in Massively Multilingual Models* (LREC-COLING, 2024)

- **Reading across languages and probabilistic information.**  
  I evaluate how model-based predictability (e.g., surprisal, cloze-related measures) relates to human reading behavior in multiple languages and test scaling effects (model size vs. early/late measures).  
  _Representative papers:_  
  – *Scaling in Cognitive Modelling: A Multilingual Approach to Human Reading Times* (ACL Short, 2023)  
  – *Cloze Probability, Predictability Ratings, and Computational Estimates…* (Behavior Research Methods, 2024)  
  – *The Effects of Surprisal Across Languages* (Findings of ACL AACL-IJCNLP, 2022)  
  – *Locally Biased Transformers Better Align with Human Reading Times* (CMCL, 2024)

- **Brain–model alignment across languages.**  
  I train encoding models to predict fMRI responses to language from MNLM representations and test zero-shot transfer across languages and families, probing language-general principles in neural responses.  
  _Representative paper:_  
  – *Multilingual Computational Models Reveal Shared Brain Responses to 21 Languages* (bioRxiv, 2025; R&R)

<p class="pub-note">
For full citations and links to journals and PDFs, see the <a href="{{ '/publications/' | relative_url }}">Publications</a> page.
</p>
